---
title: "P-hacking is Good for Business"
subtitle: "Using NumPy to show breaking the rules is profitable!"
author: "Russ Zaliznyak <[rzaliznyak@gmail.com](mailto:rzaliznyak@gmail.com)>"
date: "2025-03-14"
execute:
  echo: false
format: 
  html: 
    css: style.css
    toc: true
    toc-expand: true
    toc-indent: 1em
jupyter: python3
---

```{python}
#| code-fold: false
#| echo: false

from math import ceil, sqrt
from scipy.stats import norm


def two_proportion_required_samples(
    alpha,
    power,
    control_rate,
    treatment_rate,
    control_to_treatment_ratio=1,
    tail_type="one_tail",
):
    """Calculate the required number of samples for two-proportion test.
    :param float alpha: Value in (0,1) that specifies desired False Positive Rate
    :param float power: Value in (0,1) that specifies desired 1 - False Negative Rate
    :param float control_rate: Value in (0,1) that specifies expected control rate
    :param float treatment_rate: Value in (0,1) that specifies expected treatment rate
    :param float control_to_treatment_ratio: The ratio of control to treatment samples
    :param string tail_type: Specifies one-tail or two-tail experiment\n
        Defaults to "two_tail" if anything other than "one_tail" is given
    :return: prob_tuple: Tuple of required control samples and treatment samples
    :rtype: tuple
    """
    alpha_adjusted = (
        alpha if tail_type is None or tail_type.lower() == "one_tail" else alpha / 2
    )
    beta = 1 - power

    ##Determine how extreme z-score must be to reach statistically significant result
    z_stat_critical = norm.ppf(1 - alpha_adjusted)
    ##Determine z-score of Treatment - Control that corresponds to a True Positive Rate (1-beta)
    z_power_critical = norm.ppf(beta)
    ##Expected Difference between treatment and control
    expected_delta = treatment_rate - control_rate
    ##Control Allocation Rate
    control_allocation = control_to_treatment_ratio / (control_to_treatment_ratio + 1)
    ##Treatment Allocation Rate
    treatment_allocation = 1 - control_allocation

    ##Calculate Variance of Treatment Rate - Control Rate
    blended_p = (
        treatment_rate * treatment_allocation + control_rate * control_allocation
    )
    blended_q = 1 - blended_p
    variance_blended = (
        blended_p * blended_q / (control_allocation * treatment_allocation)
    )
    ##Total Samples Required
    total_samples_required = (
        variance_blended
        * ((z_power_critical - z_stat_critical) / (0 - expected_delta)) ** 2
    )
    ##Split total samples into control and treatment
    control_samples = ceil(control_allocation * total_samples_required)
    treatment_samples = ceil(treatment_allocation * total_samples_required)
    ##Perhaps return required_delta in the future
    required_delta = z_stat_critical * sqrt(variance_blended) + 0
    return (control_samples, treatment_samples)

```

```{python}
from IPython.display import Markdown, display, HTML
import pandas as pd


def df_to_html_with_highlighted_row(df, highlight_row=-1, column_width=150):
    # Start building the HTML table
    html = '<table style="border-collapse: collapse; width: 100%;">'

    # Add header row with fixed width
    html += "<tr>"
    for col in df.columns:
        html += f'<th style="border: 1px solid black; padding: 8px; width: {column_width}px; background-color: #f0f0f0;">{col}</th>'
    html += "</tr>"

    # Add data rows, highlighting the specified row
    for index, row in df.iterrows():
        html += "<tr"
        if index == highlight_row:
            html += ' style="background-color: yellow;"'  # Highlight the specified row
        html += ">"

        for value in row:
            html += f'<td style="border: 1px solid black; padding: 8px; width: {column_width}px;">{value}</td>'

        html += "</tr>"

    html += "</table>"

    return html




```

# Introduction

We learned in our last paper that [**p-values**](https://rzaliznyak-math.github.io/numpy-random-p-value-power/#proper-p-value-practice){target="_blank"} control the rate of `False Positives` in our experiments --- how often we incorrectly declare `Statistical Significance`.

You may have also heard that _**`p-hacking`**_, the practice of misusing p-values, should be avoided because it inflates `False Positive` rates.

While the aforementioned claim is absolutely true, here we will use _**`NumPy`**_ simulations to show that in many cases, it is profitable to engage in `p-hacking`.


# Design Significance Test

Let's use our traditional website example.

Assume our existing website converts 50% of customers to paying subscribers.<br> We have a new website we think generates a lift of 101% --- or a delta-to-control = `{python} f"{0.005:.2%}"`.

```{python}
#| label: fig-mde
#| fig-cap: "Minimum Detectable Effect (MDE)"
#| code-fold: true
#| echo: true

control_mean = 0.50
min_number_test_days = 3
max_number_test_days = 5
alpha = 0.05
power = 0.80
index_to_control = 1.01
treatment_mean_planned = control_mean * index_to_control
true_delta_planned = treatment_mean_planned - control_mean

data = {
    "Max( Test Days )": [max_number_test_days],
    "Conversion Rate": [f"{control_mean:.1%}"],
    "MDE (ITC)": [index_to_control * 100],
    "Significance (α)": [alpha],
    "Power (1 - β)": [power],
}

df = pd.DataFrame(data)

# Display the DataFrame
display(HTML(df_to_html_with_highlighted_row(df)))

```

Using the methods from our last paper, we can calculate exactly how many `control` and `treatment` samples would be needed for this experiment. 


```{python}
#| code-fold: true
#| echo: true

control_mean = 0.50
alpha = 0.05
power = 0.80
index_to_control = 1.01
treatment_mean = control_mean * index_to_control
true_delta = treatment_mean - control_mean

required_control_samples, required_treatment_samples = two_proportion_required_samples(
    alpha=alpha,
    power=power,
    control_rate=control_mean,
    treatment_rate=treatment_mean,
    control_to_treatment_ratio=1,
    tail_type="one_tail",
)

print(f"Control Samples: {required_control_samples:,}")
print(f"Treatment Samples: {required_treatment_samples:,}")

```

According to our calculations, with the above sample counts, our significance test has a 5% False Positive Rate and an 80% chance of detecting a winner.

# Our Treatment Effects

Before we go further; let's make an assumption about our `Treatment Effects`.

::: {style="margin-top: -0px; font-size: 97%"}
```{python}
#| code-fold: true
#| echo: true
#| label: fig-treament-effects
#| fig-cap: "Every treatment effect we simulate has an equal chance of being above or below 100"
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from scipy import stats

from numpy.random import seed

seed(5)


number_simulations = int(1e4)
null_average = 100
scale = 0.25
treatment_effect_samples = np.random.laplace(
    loc=null_average, scale=scale, size=number_simulations
)

x_global_min = min(treatment_effect_samples)
x_global_max = max(treatment_effect_samples)
x_range = np.linspace(x_global_min, x_global_max, 1000)


kde = stats.gaussian_kde(np.random.laplace(loc=100, scale=scale, size=50000))
y_kde = kde(x_range)

# Calculate CDF
cdf_values = stats.norm.cdf(x_range, loc=100, scale=scale)

fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=x_range,
        y=y_kde,
        mode="lines",
        fill="tozeroy",
        name=f"Scale: {scale}",
        hovertemplate="Pr(Treatment Effect > %{x:.2f}) = %{customdata:.2%}<extra></extra>",
        hoverlabel=dict(font_size=18, font_family="Arial"),
        customdata=1 - cdf_values,
    ),
    # row=row,
    # col=col
)

fig.update_xaxes(
    title_text="Potential Treatment Effect (ITC)",
    showticklabels=True,
    title_standoff=10,
)

fig.update_layout(
    title=f"Treatment Effects are Laplace Distributed with μ = {null_average} and b = {scale}",
    hoverlabel=dict(font_size=16, font_family="Arial", font_color="white"),
)

fig.update_layout(
    # height=500,  # Adjust height for 2x2 layout
    # width=700,   # Adjust width for better spacing
    showlegend=False,
    font=dict(size=14),
)
fig.update_yaxes(title_text="", showticklabels=False)
fig.update_xaxes(tickvals=[98, 99, 100, 101, 102])
fig.show()

```
:::

**Reality Check**: Though our significance test is designed to reliably detect effects at 101 or higher; our treatment effects have an equally likely chance of being above or below 100 (equality).

# Simulate Dataset
Below we simulate data for `{python} f"{number_simulations:,}"` of our previously defined significance tests. Each simulated test has a treatment effect that is randomly sampled from our `Laplace` distribution.
```{python}
#| code-fold: true
#| echo: true
#| label: fig-dataset
#| fig-cap: "The output of our last simulation; among 10,000 in total."

from numpy.random import seed, binomial, laplace
from numpy import repeat, cumsum, round, sqrt, array

null_average = 100
scale = 0.25


all_sims = []
treatment_effect_samples = laplace(
    loc=null_average, scale=scale, size=number_simulations
)

all_positive_effect_results = []
all_negative_effect_results = []

for simulation_number in range(number_simulations):
    treatment_effect_actual = treatment_effect_samples[simulation_number]

    control_daily_trials = int(required_control_samples / max_number_test_days)
    treatment_daily_trials = int(required_treatment_samples / max_number_test_days)

    control_trials_list = cumsum(repeat(control_daily_trials, max_number_test_days))
    treatment_trials_list = cumsum(repeat(treatment_daily_trials, max_number_test_days))

    treatment_mean = control_mean * treatment_effect_actual / 100
    control_events_list = cumsum(
        binomial(control_daily_trials, control_mean, max_number_test_days)
    )
    treatment_events_list = cumsum(
        binomial(treatment_daily_trials, treatment_mean, max_number_test_days)
    )

    p_control = control_events_list / control_trials_list
    p_treatment = treatment_events_list / treatment_trials_list

    delta_list = p_treatment - p_control
    observed_effect_list = (
        treatment_events_list
        * control_trials_list
        / (control_events_list * treatment_trials_list)
    )

    standard_error = sqrt(
        (p_control * (1 - p_control) / control_trials_list)
        + (p_treatment * (1 - p_treatment) / treatment_trials_list)
    )

    # Compute Z-score
    z_score = round((delta_list - 0) / standard_error, 2)
    p_values = array([1 - norm.cdf(z) for z in z_score])
    inner_dict = {
        "experiment_number": simulation_number + 1,
        "Day": [int(i + 1) for i in range(max_number_test_days)],
        "control trials": control_trials_list.astype(int),
        "control rate (%)": round(100 * p_control, 2),
        "treatment trials": treatment_trials_list,
        "treatment rate (%)": round(100 * p_treatment, 2),
        "delta (%)": round(delta_list * 100, 2),
        "z-score": z_score,
        "p-value": p_values,
        "true_effect": treatment_effect_actual,
    }
    all_sims.append(inner_dict)

    if treatment_effect_actual >= 100:
        all_positive_effect_results.append(inner_dict)
    else:
        all_negative_effect_results.append(inner_dict)

sim_df = pd.DataFrame(inner_dict)
del sim_df["p-value"]
del sim_df["true_effect"]


print(
    f"Random Treatment Effect Programmed into Treatment: {treatment_effect_actual/100:.3%}"
)


display(HTML(sim_df[sim_df.columns[1:]].to_html(index=False)))

```

# Proper Significance Test

To avoid `P-hacking`, we only consider a `p-value` at the conclusion of our test.


```{python}
#| label: fig-sig-test
#| fig-cap: "A proper significance test only checks the p-value at the conclusion of a test."
#| code-fold: true
#| echo: true

from scipy.stats import norm

p_values_display = p_values.round(3)
inner_dict["p-value"] = p_values_display
sim_df = pd.DataFrame(inner_dict)
del sim_df["true_effect"]

display(HTML(sim_df[sim_df.columns[1:]].tail(1).to_html(index=False)))
```

Let's check the results of our `{python} f"{number_simulations:,}"` simulations. Recall that 50% of the time; treatment effects are less than 100.

## Treatment Effect < 100

When our true Treatment Effect < 100, our significance test performs very well.

```{python}
#| code-fold: true
#| echo: true
from numpy import mean, where

final_df = pd.concat(map(pd.DataFrame, all_negative_effect_results), ignore_index=True)
final_df = final_df[final_df.Day == max_number_test_days]

negative_effect_accuracy = 1 - mean(where(final_df["p-value"] < 0.05, 1, 0))
```
**Retained Control**: `{python} f"{negative_effect_accuracy:.2%}"`; Treatment correctly didn't reach `Significance (α) = 0.05`.


## Treatment Effect > 100

When our true Treatment Effect > 100, our significance test performs terribly. Why? <br>Our test was designed to detect `101`, but effects are centered at 100.

```{python}
#| code-fold: true
#| echo: true

from numpy import mean, where, array

final_df = pd.concat(map(pd.DataFrame, all_positive_effect_results), ignore_index=True)
final_df = final_df[final_df.Day == max_number_test_days]

positive_effect_accuracy = mean(where(final_df["p-value"] < alpha, 1, 0))

```

**Picked Treatment**: `{python} f"{positive_effect_accuracy:.2%}"`; Treatment correctly reached `Significance (α) = 0.05`.


# Improper Significance Test

Let's break the rules!!! Let's glance at `p-value` every day of our test.<br>
_**`Significance Rule`**_: p-value < 0.05 <br>
_**`Futility Rule`**_: p-value > 0.95


```{python}
#| label: fig-bad-sig-test
#| fig-cap: "Breaking the rules of significance testing allows us more easily detect winners."
#| code-fold: true
#| echo: true
from scipy.stats import norm

# Build all sims for analysis
later_df = pd.concat(map(pd.DataFrame, all_sims), ignore_index=True)

# Calculate First Occurence of p-value < 0.05 for every simulated experiment
first_day_occurrence = (
    later_df[later_df["p-value"] < alpha].groupby("experiment_number")["Day"].min()
)
later_df["first_occurrence_sig"] = later_df["experiment_number"].map(
    first_day_occurrence
)
later_df["first_occurrence_sig"] = later_df["first_occurrence_sig"].fillna(
    max_number_test_days
)


# Calculate First Occurence of Futility p-value > 0.95 for every simulated experiment
first_day_occurrence_futility = (
    later_df[later_df["p-value"] > 1 - alpha].groupby("experiment_number")["Day"].min()
)
later_df["first_day_occurrence_futility"] = later_df["experiment_number"].map(
    first_day_occurrence_futility
)
later_df["first_day_occurrence_futility"] = later_df[
    "first_day_occurrence_futility"
].fillna(max_number_test_days)

# Determine Which Takes Place First
later_df["first_day"] = later_df[
    ["first_occurrence_sig", "first_day_occurrence_futility"]
].min(axis=1)

# Display
display(
    HTML(
        later_df[later_df.columns[1 : len(later_df.columns) - 4]]
        .tail(5)
        .to_html(index=False)
    )
)

```

## Treatment Effect < 100

When our true Treatment Effect < 100, our significance test performs a little worse.

```{python}
#| code-fold: true
#| echo: true

# Shrink datasets to only final day records
small_df = later_df[later_df.Day == later_df.first_day]

# Zoom into experiments where true effect < 100
negative_df = small_df[small_df.true_effect < 100]

# Calculate runtime of experiment
avg_number_days = negative_df["Day"].mean()

# Calculate accuracy of retaining control
negative_effect_accuracy = 1 - mean(where(negative_df["p-value"] < alpha, 1, 0))
```
**Retained Control**: `{python} f"{negative_effect_accuracy:.2%}"`; Treatment correctly didn't reach `Significance (α) = 0.05`. <br>
**Avg runtime**: `{python} f"{avg_number_days:.2f}"`



## Treatment Effect > 100

When our true Treatment Effect > 100, our Improper significance test performs better than a proper one.
And it takes 20% less time to confirm a winner.

```{python}
#| code-fold: true
#| echo: true
from numpy import mean, where, array

small_df = later_df[later_df.Day == later_df.first_day]

positive_df = small_df [small_df.true_effect > 100]

avg_number_days = positive_df["Day"].mean()

positive_effect_accuracy = mean(where(positive_df["p-value"] < alpha , 1, 0))

```

**Picked Treatment**: `{python} f"{positive_effect_accuracy:.2%}"`; Treatment correctly reached `Significance (α) = 0.05`. <br>
**Avg runtime**: `{python} f"{avg_number_days:.2f}"`